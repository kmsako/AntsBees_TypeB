{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TypeA_AntsBees_VGG16_PyTorch_file_manipulation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Vep4RNcQcNNF"},"source":["\n","#1. アリとハチの画像データをダウンロードして解凍する\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YyICRi2gcHJc","executionInfo":{"status":"ok","timestamp":1640249643448,"user_tz":-540,"elapsed":16355,"user":{"displayName":"けん","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05058399355894861134"}},"outputId":"a4448f9a-49f6-4ac0-876c-c0a20e939011"},"source":["%pwd\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"tQ6qsCS_hJZ1"},"source":["data_dir = '/content/gdrive/My Drive/Colab Notebooks/AntsBees/data'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MW_mRujS51zl","colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"status":"error","timestamp":1640249701879,"user_tz":-540,"elapsed":52643,"user":{"displayName":"けん","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05058399355894861134"}},"outputId":"244b1bdc-8022-4bd6-8a4d-878f80e21c84"},"source":["'''\n","1. アリとハチの画像データをダウンロードして解凍する\n","'''\n","# PyTorchのチュートリアル\n","# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n","# で用意されているデータセットを利用\n","import os\n","import urllib.request\n","import zipfile\n","\n","\n","# 指定したフォルダーが存在しない場合は作成する\n","if not os.path.exists(data_dir):\n","    os.mkdir(data_dir)\n","# アリとハチの画像のダウンロード先\n","url = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\n","# フォルダーのディレクトリにファイル名を連結してパスを作成\n","save_path = os.path.join(data_dir, 'hymenoptera_data.zip')\n","# ZIPファイルを解答して保存\n","if not os.path.exists(save_path):\n","    urllib.request.urlretrieve(url, save_path) # ZIPファイルを取得\n","    zip = zipfile.ZipFile(save_path) # ZIPファイルを読み込む\n","    zip.extractall(data_dir) # ZIPファイルを解凍\n","    zip.close()              # ZIPファイルをクローズ\n","    os.remove(save_path)     # ZIPファイルを消去"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-23d8d7d6796e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ZIPファイルを取得\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mzip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ZIPファイルを読み込む\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ZIPファイルを解凍\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# ZIPファイルをクローズ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# ZIPファイルを消去\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"zoAy0ogDQwVv"},"source":["'''\n","2. 前処理クラスの定義\n","'''\n","class ImageTransform():\n","    '''画像の前処理クラス。訓練時、検証時で異なる動作をする。\n","\n","    Attributes:\n","      data_transform(dic):\n","        train: 訓練用のトランスフォーマーオブジェクト\n","        val  : 検証用のトランスフォーマーオブジェクト\n","    '''\n","\n","    def __init__(self, resize, mean, std):\n","        '''トランスフォーマーオブジェクトを生成する。\n","\n","        Parameters:\n","        resize(int): リサイズ先の画像の大きさ\n","        mean(tuple): (R, G, B)各色チャネルの平均値\n","        std        : (R, G, B)各色チャネルの標準偏差\n","        '''\n","        # dicに訓練用、検証用のトランスフォーマーを生成して格納\n","        self.data_transform = {\n","            'train': transforms.Compose([\n","                # ランダムにトリミングする\n","                transforms.RandomResizedCrop(\n","                    resize, # トリミング後の出力サイズ\n","                    scale=(0.5, 1.0)),  # スケールの変動幅\n","                transforms.RandomHorizontalFlip(p = 0.5),  # 0.5の確率で左右反転\n","                transforms.RandomRotation(15),  # 15度の範囲でランダムに回転\n","                transforms.ToTensor(),          # Tensorオブジェクトに変換\n","                transforms.Normalize(mean, std) # 標準化\n","            ]),\n","            'val': transforms.Compose([\n","                transforms.Resize(resize),      # リサイズ\n","                transforms.CenterCrop(resize),  # 画像中央をresize×resizeでトリミング\n","                transforms.ToTensor(),          # テンソルに変換\n","                transforms.Normalize(mean, std) # 標準化\n","            ])\n","        }\n","\n","    def __call__(self, img, phase='train'):\n","        '''オブジェクト名でコールバックされる\n","        Parameters:\n","          img: 画像\n","          phase(str): 'train'または'val' 前処理のモード\n","        '''\n","        return self.data_transform[phase](img) # phaseはdictのキー"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":370},"id":"r0hjBfLZRNlM","executionInfo":{"status":"error","timestamp":1640249713260,"user_tz":-540,"elapsed":6087,"user":{"displayName":"けん","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05058399355894861134"}},"outputId":"b5f136c5-0ae8-41fe-f38c-98d67915e33b"},"source":["'''\n","3. 前処理前後の画像を確認する\n","'''\n","import numpy as np\n","from PIL import Image\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# サンプル画像を1枚読み込む\n","image_file_path = './data/hymenoptera_data/train/bees/2405441001_b06c36fa72.jpg'\n","img = Image.open(image_file_path)   # (高さ, 幅, RGB)\n","\n","# 元の画像の表示\n","plt.imshow(img)\n","plt.show()\n","\n","# 画像の前処理と処理済み画像の表示\n","# モデルの入力サイズ(タテ・ヨコ)\n","SIZE = 224\n","# 標準化する際の各RGBの平均値\n","MEAN = (0.485, 0.456, 0.406) # ImageNetデータセットの平均値を使用\n","# 標準化する際の各RGBの標準偏差\n","STD = (0.229, 0.224, 0.225)  # ImageNetデータセットの標準偏差を使用\n","\n","# トランスフォーマーオブジェクトを生成\n","transform = ImageTransform(SIZE, MEAN, STD)\n","# 訓練モードの前処理を適用、torch.Size([3, 224, 224])\n","img_transformed = transform(img, phase=\"train\")\n","\n","# (色、高さ、幅)を (高さ、幅、色)に変換\n","img_transformed = img_transformed.numpy().transpose((1, 2, 0))\n","# ピクセル値を0～1の範囲に制限して表示\n","img_transformed = np.clip(img_transformed, 0, 1)\n","plt.imshow(img_transformed)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-61be9124529a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# サンプル画像を1枚読み込む\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mimage_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/hymenoptera_data/train/bees/2405441001_b06c36fa72.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_file_path\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (高さ, 幅, RGB)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 元の画像の表示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/hymenoptera_data/train/bees/2405441001_b06c36fa72.jpg'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R01r-VZqRdpM","executionInfo":{"status":"ok","timestamp":1611574217844,"user_tz":-540,"elapsed":909,"user":{"displayName":"Main Toshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcOIVKs4kKB2bkZdejxUGXGPjbOQBCknqRDw23=s64","userId":"01369137257200460524"}},"outputId":"ce839b21-9ad3-4437-ff3e-b2f3629bf359"},"source":["'''\n","4.  アリとハチの画像のファイルパスをリストにする\n","'''\n","import os.path as osp\n","import glob\n","import pprint\n","\n","def make_datapath_list(phase=\"train\"):\n","    '''\n","    データのファイルパスを格納したリストを作成する。\n","\n","    Parameters:\n","      phase(str): 'train'または'val'\n","\n","    Returns:\n","      path_list(list): 画像データのパスを格納したリスト\n","    '''\n","    # 画像ファイルのルートディレクトリ\n","    rootpath = \"./data/hymenoptera_data/\"\n","    # 画像ファイルパスのフォーマットを作成\n","    # rootpath +\n","    #   train/ants/*.jpg\n","    #   train/bees/*.jpg\n","    #   val/ants/*.jpg\n","    #   val/bees/*.jpg\n","    target_path = osp.join(rootpath + phase + '/**/*.jpg')\n","    # ファイルパスを格納するリスト\n","    path_list = []  # ここに格納する\n","\n","    # glob()でファイルパスを取得してリストに追加\n","    for path in glob.glob(target_path):\n","        path_list.append(path)\n","\n","    return path_list\n","\n","# ファイルパスのリストを生成\n","train_list = make_datapath_list(phase=\"train\")\n","val_list = make_datapath_list(phase=\"val\")\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train\n","['./data/hymenoptera_data/train/ants/45472593_bfd624f8dc.jpg',\n"," './data/hymenoptera_data/train/ants/1473187633_63ccaacea6.jpg',\n"," './data/hymenoptera_data/train/ants/6240338_93729615ec.jpg',\n"," './data/hymenoptera_data/train/ants/2265825502_fff99cfd2d.jpg',\n"," './data/hymenoptera_data/train/ants/522415432_2218f34bf8.jpg']\n","['./data/hymenoptera_data/train/bees/2625499656_e3415e374d.jpg',\n"," './data/hymenoptera_data/train/bees/2959730355_416a18c63c.jpg',\n"," './data/hymenoptera_data/train/bees/3090975720_71f12e6de4.jpg',\n"," './data/hymenoptera_data/train/bees/774440991_63a4aa0cbe.jpg',\n"," './data/hymenoptera_data/train/bees/2470492904_837e97800d.jpg']\n","val\n","['./data/hymenoptera_data/val/ants/17081114_79b9a27724.jpg',\n"," './data/hymenoptera_data/val/ants/2255445811_dabcdf7258.jpg',\n"," './data/hymenoptera_data/val/ants/239161491_86ac23b0a3.jpg',\n"," './data/hymenoptera_data/val/ants/Hormiga.jpg',\n"," './data/hymenoptera_data/val/ants/11381045_b352a47d8c.jpg']\n","['./data/hymenoptera_data/val/bees/2506114833_90a41c5267.jpg',\n"," './data/hymenoptera_data/val/bees/540976476_844950623f.jpg',\n"," './data/hymenoptera_data/val/bees/2321144482_f3785ba7b2.jpg',\n"," './data/hymenoptera_data/val/bees/57459255_752774f1b2.jpg',\n"," './data/hymenoptera_data/val/bees/2086294791_6f3789d8a6.jpg']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CBxJOJZce1ac"},"source":["#(1)　DataSetの作成"]},{"cell_type":"code","metadata":{"id":"aK-1tefdRsqk"},"source":["'''\n","5. アリとハチの画像のデータセットを作成するクラス\n","'''\n","import torch.utils.data as data\n","\n","class MakeDataset(data.Dataset):\n","    '''\n","    アリとハチの画像のDatasetクラス\n","    PyTorchのDatasetクラスを継承\n","\n","    Attributes:\n","      file_list(list): 画像のパスを格納したリスト\n","      transform(object): 前処理クラスのインスタンス\n","      phase(str): 'train'または'val'\n","    Returns:\n","      img_transformed: 前処理後の画像データ\n","      label(int): 正解ラベル\n","    '''\n","    def __init__(self, file_list, transform=None, phase='train'):\n","        '''インスタンス変数の初期化\n","        '''\n","        self.file_list = file_list  # ファイルパスのリスト\n","        self.transform = transform  # 前処理クラスのインスタンス\n","        self.phase = phase          # 'train'または'val'\n","\n","    def __len__(self):\n","        '''len(obj)で実行されたときにコールされる関数\n","        画像の枚数を返す'''\n","        return len(self.file_list)\n","\n","    def __getitem__(self, index):\n","        '''Datasetクラスの__getitem__()をオーバーライド\n","           obj[i]のようにインデックスで指定されたときにコールバックされる\n","\n","           Parameters:\n","             index(int): データのインデックス\n","           Returns:\n","\n","          前処理をした画像のTensor形式のデータとラベルを取得\n","        '''\n","\n","        # ファイルパスのリストからindex番目の画像をロード\n","        img_path = self.file_list[index]\n","        # ファイルを開く -> (高さ, 幅, RGB)\n","        img = Image.open(img_path)\n","\n","        # 画像を前処理  -> torch.Size([3, 224, 224])\n","        img_transformed = self.transform(\n","            img, self.phase)\n","\n","        # 正解ラベルをファイル名から切り出す\n","        if self.phase == 'train':\n","            # 訓練データはファイルパスの31文字から34文字が'ants'または'bees'\n","            label = img_path[30:34]\n","        elif self.phase == 'val':\n","            # 検証データはファイルパスの29文字から32文字が'ants'または'bees'\n","            label = img_path[28:32]\n","\n","        # 正解ラベルの文字列を数値に変更する\n","        if label == 'ants':\n","            label = 0 # アリは0\n","        elif label == 'bees':\n","            label = 1 # ハチは1\n","\n","        return img_transformed, label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Ky6U40rfB8C"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZKjqGyffZqG"},"source":["#（２）DataLoader の作成"]},{"cell_type":"code","metadata":{"id":"oou2pgRvSDaK"},"source":["'''\n","6. データローダーの生成\n","'''\n","import torch\n","\n","# ミニバッチのサイズを指定\n","batch_size = 32\n","# 画像のサイズ、平均値、標準偏差の定数値\n","size, mean, std = SIZE, MEAN, STD\n","\n","# MakeDatasetで前処理後の訓練データと正解ラベルを取得\n","train_dataset = MakeDataset(\n","    file_list=train_list, # 訓練データのファイルパス\n","    transform=ImageTransform(size, mean, std), # 前処理後のデータ\n","    phase='train')\n","# MakeDatasetで前処理後の検証データと正解ラベルを取得\n","val_dataset = MakeDataset(\n","    file_list=val_list, # 検証データのファイルパス\n","    transform=ImageTransform(size, mean, std), # 前処理後のデータ\n","    phase='val')\n","\n","# 訓練用のデータローダー:(バッチサイズ, 3, 224, 224)を生成\n","train_dataloader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=batch_size, shuffle=True)\n","# 検証用のデータローダー:(バッチサイズ, 3, 224, 224)を生成\n","val_dataloader = torch.utils.data.DataLoader(\n","    val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# データローダーをdictにまとめる\n","dataloaders = {'train': train_dataloader, 'val': val_dataloader}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7xokL9CTMNC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611577007825,"user_tz":-540,"elapsed":2892,"user":{"displayName":"Main Toshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcOIVKs4kKB2bkZdejxUGXGPjbOQBCknqRDw23=s64","userId":"01369137257200460524"}},"outputId":"e019b9f4-b3ba-4414-839d-7c3b6f29c5ce"},"source":["'''\n","7. 学習済みのVGG16モデルをロード\n","'''\n","from torchvision import models\n","import torch.nn as nn\n","\n","# ImageNetで事前トレーニングされたVGG16モデルを取得\n","model = models.vgg16(pretrained=True)\n","\n","# VGG16の出力層のユニット数を2にする\n","model.classifier[6] = nn.Linear(\n","    in_features=4096, # 入力サイズはデフォルトの4096\n","    out_features=2)   # 出力はデフォルトの1000から2に変更\n","\n","# 使用可能なデバイス(CPUまたはGPU）を取得する\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","print(model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=2, bias=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"szb6HJ4WTi2P","executionInfo":{"status":"ok","timestamp":1611578198408,"user_tz":-540,"elapsed":1295,"user":{"displayName":"Main Toshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcOIVKs4kKB2bkZdejxUGXGPjbOQBCknqRDw23=s64","userId":"01369137257200460524"}},"outputId":"fa14fb72-3e38-49aa-aac9-8d68503e5fa6"},"source":["'''\n","8. VGG16で学習可能にする層を設定\n","'''\n","# 転移学習で学習させるパラメータを、変数params_to_updateに格納する\n","params_to_update = []\n","\n","# 出力層の重みとバイアスを更新可として登録\n","update_param_names = ['classifier.6.weight', 'classifier.6.bias']\n","\n","# 出力層以外は勾配計算をなくし、変化しないように設定\n","for name, param in model.named_parameters():\n","    if name in update_param_names:\n","        param.requires_grad = True # 勾配計算を行う\n","        params_to_update.append(param) # パラメーター値を更新\n","        print(name) # 更新するパラメーター名を出力\n","    else:\n","        param.requires_grad = False # 出力層以外は勾配計算なし"],"execution_count":null,"outputs":[{"output_type":"stream","text":["classifier.6.weight\n","classifier.6.bias\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HlpwzhUDTP1_"},"source":["'''\n","9. 損失関数とオプティマイザーを生成\n","'''\n","import torch.optim as optim\n","\n","# 損失関数\n","criterion = nn.CrossEntropyLoss()\n","# オプティマイザー\n","optimizer = optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"93qz4fdYTrmP"},"source":["'''\n","10.  学習を行う関数の定義\n","'''\n","from tqdm import tqdm\n","\n","def train_model(model, dataloaders, criterion, optimizer, num_epochs):\n","    '''モデルを使用して学習を行う\n","\n","    Parameters:\n","      model: モデルのオブジェクト\n","      dataloaders(dict): 訓練、検証のデータローダー\n","      criterion: 損失関数\n","      optimizer: オプティマイザー\n","      num_epochs: エポック数\n","    '''\n","    # epochの数だけ\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","        print('-------------')\n","\n","        # 学習と検証のループ\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # モデルを訓練モードにする\n","            else:\n","                model.eval()   # モデルを検証モードにする\n","\n","            epoch_loss = 0.0    # 1エポックあたりの損失の和\n","            epoch_corrects = 0  # 1エポックあたりの精度の和\n","\n","            # 未学習時の検証性能を確かめるため、epoch=0の学習は行わない\n","            if (epoch == 0) and (phase == 'train'):\n","                continue\n","\n","            # 1ステップにおける訓練用ミニバッチを使用した学習\n","            # tqdmでプログレスバーを表示する\n","            for inputs, labels in tqdm(dataloaders[phase]):\n","                # torch.Tensorオブジェクトにデバイスを割り当てる\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                # オプティマイザーを初期化\n","                optimizer.zero_grad()\n","                # 順伝搬（forward）計算\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs) # モデルの出力を取得\n","                    # 出力と正解ラベルの誤差から損失を取得\n","                    loss = criterion(outputs, labels)\n","                    # 出力された要素数2のテンソルの最大値を取得\n","                    _, preds = torch.max(outputs, dim=1)\n","                    \n","                    # 訓練モードではバックプロパゲーション\n","                    if phase == 'train':\n","                        loss.backward() # 逆伝播の処理(自動微分による勾配計算)\n","                        optimizer.step() # 勾配降下法でバイアス、重みを更新\n","\n","                    # ステップごとの損失を加算、inputs.size(0)->32\n","                    epoch_loss += loss.item() * inputs.size(0)\n","                    # ステップごとの精度を加算\n","                    epoch_corrects += torch.sum(preds == labels.data)\n","\n","            # エポックごとの損失と精度を表示\n","            epoch_loss = epoch_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = epoch_corrects.double(\n","                ) / len(dataloaders[phase].dataset)\n","\n","            # 出力\n","            print('{} - loss: {:.4f} - acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2UXLkMy9Tvtf","executionInfo":{"status":"ok","timestamp":1610338473692,"user_tz":-540,"elapsed":19842,"user":{"displayName":"Main Toshi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcOIVKs4kKB2bkZdejxUGXGPjbOQBCknqRDw23=s64","userId":"01369137257200460524"}},"outputId":"d2e8739a-bc5e-49e2-bac9-0a7a8c49305a"},"source":["%%time\n","'''\n","11.  学習・検証を実行する\n","'''\n","num_epochs=3\n","train_model(model, dataloaders, criterion, optimizer, num_epochs=num_epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/3\n","-------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 5/5 [00:02<00:00,  2.44it/s]\n","  0%|          | 0/8 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["val - loss: 0.8348 - acc: 0.2222\n","Epoch 2/3\n","-------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 8/8 [00:02<00:00,  2.75it/s]\n","  0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["train - loss: 0.6424 - acc: 0.6008\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 5/5 [00:01<00:00,  2.55it/s]\n","  0%|          | 0/8 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["val - loss: 0.2150 - acc: 0.9412\n","Epoch 3/3\n","-------------\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 8/8 [00:02<00:00,  2.80it/s]\n","  0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["train - loss: 0.1743 - acc: 0.9506\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 5/5 [00:01<00:00,  2.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["val - loss: 0.1241 - acc: 0.9542\n","CPU times: user 9.53 s, sys: 2.2 s, total: 11.7 s\n","Wall time: 11.7 s\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wiIJJo1bT0kG"},"source":[""],"execution_count":null,"outputs":[]}]}